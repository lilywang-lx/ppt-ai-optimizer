# 快速开始指南

## 方式一：使用启动脚本（推荐）

### 1. 配置API Key

编辑配置文件 `backend/config/config.yaml`，填写各模型的API Key：

```yaml
models:
  xunfei:
    app_id: "你的APP_ID"
    api_key: "你的API_KEY"
    api_secret: "你的API_SECRET"

  wenxin:
    api_key: "你的API_KEY"
    secret_key: "你的SECRET_KEY"

  qianwen:
    api_key: "你的API_KEY"

  hunyuan:
    secret_id: "你的SECRET_ID"
    secret_key: "你的SECRET_KEY"
```

### 2. 运行启动脚本

```bash
./start.sh
```

脚本会自动：
- 安装后端Python依赖
- 安装前端Node.js依赖（如果有Node.js）
- 启动后端服务(端口8000)
- 启动前端服务(端口5173)

### 3. 访问系统

- 前端页面: http://localhost:5173
- 后端API文档: http://localhost:8000/docs

## 方式二：手动启动

### 后端启动

```bash
cd backend

# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 安装依赖
pip install -r requirements.txt

# 配置API Key (编辑 config/config.yaml)

# 启动服务
python main.py
```

### 前端启动

```bash
cd frontend

# 安装依赖
npm install

# 启动开发服务器
npm run dev
```

## 方式三：Docker部署

### 1. 构建并启动

```bash
# 构建镜像
docker-compose build

# 启动服务
docker-compose up -d

# 查看日志
docker-compose logs -f
```

### 2. 访问系统

- 前端页面: http://localhost
- 后端API: http://localhost:8000

### 3. 停止服务

```bash
docker-compose down
```

## 使用流程

### 1. 上传PPT

- 拖拽或点击上传按钮选择`.pptx`文件
- 文件大小限制：50MB
- 点击"开始优化"按钮

### 2. 等待处理

系统会显示实时进度：
- 解析PPT (10%)
- 模型分析 (30%)
- 迭代修正 (60%)
- 生成PPT (80%)
- 完成 (100%)

处理时间取决于PPT复杂度，通常需要1-3分钟。

### 3. 查看结果

处理完成后，可以：
- 查看各模型的原始建议
- 查看最终优化方案
- 查看冲突调和记录
- 下载优化后的PPT

### 4. 下载文件

点击"下载优化后的PPT"按钮，获取优化后的文件。

## 功能说明

### 多模型协同

系统集成4个PPT专用大模型：
1. **讯飞星火** - 场景化适配专家
2. **文心一言** - 排版设计专家
3. **通义千问** - 内容逻辑专家
4. **腾讯混元** - 多建议融合专家

### 优化维度

- **content**: 内容优化
- **logic**: 逻辑优化
- **layout**: 排版优化
- **color**: 配色优化
- **font**: 字体优化
- **chart**: 图表优化

### 迭代修正

系统会自动进行跨模型迭代修正：
1. 讯飞星火生成初始建议
2. 文心一言修正排版和配色
3. 通义千问修正内容和图表
4. 腾讯混元融合所有建议

### 冲突调和

当模型建议冲突时：
- 冲突率 < 5%：自动调和
- 冲突率 5%-10%：模型仲裁
- 冲突率 > 10%：提示人工选择

## 配置说明

### 迭代轮次配置

编辑 `config/config.yaml`:

```yaml
iteration:
  max_rounds: 2  # 最大迭代轮次
  conflict_threshold: 0.05  # 冲突率阈值
  manual_threshold: 0.10  # 人工干预阈值
```

### 模型启用/禁用

```yaml
models:
  xunfei:
    enabled: true  # 设置为false可禁用此模型
```

### 上传限制

```yaml
upload:
  max_size: 52428800  # 50MB
  allowed_extensions: [".pptx"]
```

## 故障排查

### 后端启动失败

1. 检查Python版本 (需要3.9+)
2. 检查依赖是否安装完整
3. 检查配置文件格式是否正确
4. 查看日志文件: `backend/logs/app.log`

### 前端启动失败

1. 检查Node.js版本 (需要16+)
2. 删除 `node_modules` 重新安装
3. 检查端口5173是否被占用

### 模型调用失败

1. 检查API Key是否正确
2. 检查网络连接
3. 查看具体错误信息
4. 系统会自动降级,跳过失败的模型

### Docker部署问题

1. 检查Docker和Docker Compose版本
2. 确保端口80和8000未被占用
3. 查看容器日志: `docker-compose logs`

## 性能优化

### 提升处理速度

1. 减少迭代轮次
2. 禁用部分模型
3. 增加API超时时间

### 减少资源占用

1. 限制并发处理数量
2. 定期清理临时文件
3. 调整日志级别

## API文档

访问 http://localhost:8000/docs 查看完整API文档。

主要接口：
- `POST /api/upload` - 上传PPT
- `GET /api/status/{ppt_id}` - 查询状态
- `GET /api/result/{ppt_id}` - 获取结果
- `GET /api/download/{ppt_id}` - 下载文件

## 扩展开发

### 新增模型

1. 在 `model_engine.py` 中创建新的模型客户端类
2. 实现 `analyze_ppt` 方法
3. 在配置文件中添加模型配置
4. 更新迭代流程配置

详见：`docs/扩展指南.md`

## 常见问题

**Q: 支持哪些PPT格式？**
A: 目前仅支持 .pptx 格式(Office 2007及以上)

**Q: 处理需要多长时间？**
A: 通常1-3分钟，取决于PPT复杂度和模型响应速度

**Q: 是否支持批量处理？**
A: 当前版本不支持，后续版本会添加

**Q: 数据是否安全？**
A: 文件仅在服务器临时存储，处理完成后可手动清理

**Q: 可以只使用部分模型吗？**
A: 可以，在配置文件中设置 `enabled: false` 禁用模型

## 联系支持

- GitHub Issues: https://github.com/lilywang-lx/ppt-ai-optimizer/issues
- 邮箱: lilywang@lexin.com

## 更新日志

### v1.0.0 (2026-01-16)
- 首次发布
- 支持4个大模型协同
- 实现跨模型迭代修正
- 完整的Web界面
